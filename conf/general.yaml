training:
  start_epoch: 0
  max_epochs: 10
  max_steps: 100000
  precision: 16
  learning_rate: 0.0003
  gradient_clip_val: 1.0
  limit_train_batches: 1.0
  weight_decay: 0.0001
  lr_scheduler:
    use: True
    total_steps: ${..max_steps}
    pct_start: 0.005
    div_factor: 25 # init_lr = max_lr / div_factor
    final_div_factor: 10000 # final_lr = max_lr / final_div_factor (this is different from Pytorch's OneCycleLR param)

validation:
  limit_val_batches: 1.0
  val_check_interval: 100 # Optional[int]
  check_val_every_n_epoch: null # Optional[int]

batch_size:
  train: 7
  eval: 7
hardware:
  num_workers:
    train: 2
    eval: 2
  gpus: [1] # Either a single integer (e.g. 3) or a list of integers (e.g. [3,5,6])
  dist_backend: "nccl"
  pin_memory: True
logging:
  ckpt_every_n_epochs: 1
  train:
    metrics:
      compute: false
      detection_metrics_every_n_steps: null # Optional[int] -> null: every train epoch, int: every N steps
    log_model_every_n_steps: 5000
    log_every_n_steps: 500 # 500
    high_dim:
      enable: True
      every_n_steps: 5000
      n_samples: 4
  validation:
    high_dim:
      enable: True
      every_n_epochs: 1
      n_samples: 8